---
title: "Hypothesis Testing in R"
author: "Nathan Ormond"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Set seed for any random processes
set.seed(123)
```

# Introduction to Hypothesis Testing in R

Statistical hypothesis testing is a fundamental approach in data analysis that allows researchers to make inferences about populations based on sample data. It provides a framework for determining whether observed differences or patterns in data represent genuine effects or could reasonably be attributed to random chance.

> **Core concept**: Hypothesis testing involves formulating a null hypothesis (typically assuming no effect) and an alternative hypothesis, then evaluating the evidence to determine which is more consistent with the observed data.

This document demonstrates how to perform common hypothesis tests in R, including:

1. Unpaired two-sample t-test
2. Paired two-sample t-test
3. Analysis of Variance (ANOVA)

## Unpaired Two-Sample t-Test

The unpaired two-sample t-test compares means from two independent groups. This test is appropriate when:

- Data from both groups approximately follow normal distributions
- The samples are independent (no natural pairing)
- We want to test if the population means differ

### Example: Comparing Male and Female Pulse Rates

Let's analyse pulse rate measurements from 10 males and 10 females to determine if there's a significant difference between genders.

#### Data Preparation

We can import our data in two different formats: wide or long. The long format is generally preferred for most statistical analyses in R.

```{r pulse_data}
# Method 1: Data in wide format
pulse_wide <- data.frame(
  Males = c(65, 62, 63, 64, 64, 68, 66, 62, 62, 63),
  Females = c(64, 67, 66, 67, 66, 69, 63, 67, 62, 65)
)

# Method 2: Data in long format (preferred for analysis)
pulse_long <- data.frame(
  Sex = c(rep("Female", 10), rep("Male", 10)),
  Pulse = c(64, 67, 66, 67, 66, 69, 63, 67, 62, 65,
            65, 62, 63, 64, 64, 68, 66, 62, 62, 63)
)

# Quick summary statistics
tapply(pulse_long$Pulse, pulse_long$Sex, summary)
```

Let's visualize the data to get a better understanding of the distributions:

```{r pulse_viz, fig.height=4, fig.width=8}
# Create side-by-side boxplots
boxplot(Pulse ~ Sex, data = pulse_long, 
        col = c("pink", "lightblue"),
        main = "Pulse Rates by Gender",
        ylab = "Pulse Rate")

# Add individual data points
stripchart(Pulse ~ Sex, data = pulse_long, 
           vertical = TRUE, method = "jitter", 
           pch = 16, col = c("red", "blue"), 
           add = TRUE)
```

#### Two-Sided t-Test (Equal Variances)

For our hypothesis test, we'll formulate:

- Null hypothesis (H₀): μₘ = μₑ (male and female population means are equal)
- Alternative hypothesis (Hₐ): μₘ ≠ μₑ (the population means differ)

We'll test this at the 5% significance level (α = 0.05).

```{r two_sided_test}
# Using wide format data
t_test_wide <- t.test(pulse_wide$Males, pulse_wide$Females, 
                       alternative = "two.sided", 
                       var.equal = TRUE)

# Using long format data (preferred approach)
t_test_long <- t.test(Pulse ~ Sex, data = pulse_long, 
                       alternative = "two.sided", 
                       var.equal = TRUE)

# Display results from long format analysis
t_test_long
```

#### Interpretation

The p-value of `r round(t_test_long$p.value, 3)` is greater than our significance level of 0.05. Therefore, we fail to reject the null hypothesis. The data does not provide sufficient evidence to conclude that male and female pulse rates differ significantly.

Note that the t-statistic is `r round(t_test_long$statistic, 3)` with `r t_test_long$parameter` degrees of freedom. The sample means are:
- Females: `r mean(pulse_wide$Females)` beats per minute
- Males: `r mean(pulse_wide$Males)` beats per minute

The 95% confidence interval for the difference in means (Female - Male) is [`r round(t_test_long$conf.int[1], 2)`, `r round(t_test_long$conf.int[2], 2)`]. Since this interval contains zero, it's consistent with our conclusion that there's no significant difference.

#### One-Sided Alternative

We might have a directional hypothesis, such as female pulse rates being higher than male pulse rates:

- Null hypothesis (H₀): μₑ = μₘ
- Alternative hypothesis (Hₐ): μₑ > μₘ

```{r one_sided_test}
t_test_one_sided <- t.test(Pulse ~ Sex, data = pulse_long, 
                           alternative = "greater", 
                           var.equal = TRUE)
t_test_one_sided
```

In this case, the p-value of `r round(t_test_one_sided$p.value, 3)` is less than our significance level of 0.05. Therefore, we reject the null hypothesis and conclude that female pulse rates are significantly higher than male pulse rates.

> **Important insight**: One-sided tests have greater power to detect an effect in the specified direction but cannot detect effects in the opposite direction. The choice between one-sided and two-sided tests should be made before seeing the data, based on your research hypothesis.

## Paired Two-Sample t-Test

The paired t-test is used when observations in the two groups are naturally paired. This test examines the differences within each pair rather than comparing group means directly.

### Example: Comparing Mathematics and Physics Scores

Let's analyse a dataset where seven students took exams in both Mathematics and Physics to determine if there's a difference in performance between subjects.

```{r grades_data}
# Create the dataset
grades <- data.frame(
  Student = 1:7,
  Maths = c(52, 48, 62, 70, 68, 42, 57),
  Physics = c(47, 63, 71, 75, 82, 45, 57)
)

# Show the data
grades

# Create a long format version for visualization
grades_long <- reshape(grades, 
                      direction = "long",
                      varying = c("Maths", "Physics"),
                      v.names = "Score",
                      timevar = "Subject",
                      times = c("Maths", "Physics"),
                      idvar = "Student")

# Remove the redundant row names
rownames(grades_long) <- NULL
```

Let's visualize the paired nature of our data:

```{r grades_viz, fig.height=4, fig.width=9}
# Side-by-side boxplot
par(mfrow = c(1, 2))

# Boxplot
boxplot(Score ~ Subject, data = grades_long,
        col = c("lightgreen", "lightyellow"),
        main = "Subject Scores",
        ylab = "Score")

# Paired line plot showing the individual differences
plot(1:7, grades$Maths, type = "b", pch = 16, col = "darkgreen",
     ylim = c(40, 85), xlab = "Student", ylab = "Score",
     main = "Paired Scores by Student")
points(1:7, grades$Physics, type = "b", pch = 17, col = "orange")
legend("bottomright", legend = c("Maths", "Physics"),
       pch = c(16, 17), col = c("darkgreen", "orange"))
```

The line plot clearly shows the paired nature of the data - each student has a score in both subjects.

#### Formulating Hypotheses

- Null hypothesis (H₀): μₘ = μₚ (no difference between subjects)
- Alternative hypothesis (Hₐ): μₘ < μₚ (Physics scores are higher than Maths scores)

```{r paired_test}
# Method 1: Using wide format with paired=TRUE
t_test_paired_wide <- t.test(grades$Maths, grades$Physics, 
                             paired = TRUE, 
                             alternative = "less")

# Method 2: For paired t-tests with long format data, we need a different approach
# We can't use the formula interface with paired=TRUE directly
# Instead, we can reshape the data or use a different method

# Display results
t_test_paired_wide
```

> **Note**: When working with paired data in long format, the standard formula interface 
> `t.test(Score ~ Subject, data = grades_long, paired = TRUE)` won't work. 
> The formula interface doesn't know how to pair the observations correctly.
> Instead, you should either:
> 1. Use wide format data where each row represents a subject with columns for each condition
> 2. Create a proper paired design with subject IDs and use another package like `lme4` or `nlme`
> 3. Calculate differences manually and perform a one-sample t-test
#### Interpretation

The p-value of `r round(t_test_paired_wide$p.value, 3)` is less than our significance level of 0.05. Therefore, we reject the null hypothesis and conclude that Physics scores are significantly higher than Mathematics scores.

The mean difference (Maths - Physics) is `r round(t_test_paired_wide$estimate, 2)` points. The negative sign indicates that Physics scores are higher on average.

> **Statistical insight**: The paired t-test has greater power than an unpaired test when there's a natural pairing because it removes between-subject variability, focusing only on the within-pair differences.

#### Manual Calculation of Differences

We can also explicitly calculate the differences and perform a one-sample t-test on them, which is mathematically equivalent to the paired t-test:

```{r manual_paired}
# Calculate differences (Maths - Physics)
grades$Difference <- grades$Maths - grades$Physics

# Perform one-sample t-test on the differences
t.test(grades$Difference, mu = 0, alternative = "less")
```

## Analysis of Variance (ANOVA)

ANOVA extends hypothesis testing to compare means across more than two groups. It tests whether the means of several groups are all equal against the alternative that at least one mean differs.

### Example: Upper Body Power with Different Ski Pole Grips

Let's analyse a study that examined upper body power (UBP) using three types of ski pole grips.

```{r grip_data}
# Create the dataset
grip_data <- data.frame(
  ubp = c(168.2, 161.4, 163.2, 166.7, 173.0, 173.3, 160.1, 161.2, 166.8),
  grip = factor(c(rep("classic", 3), rep("integrated", 3), rep("modern", 3)))
)

# Summary statistics by group
tapply(grip_data$ubp, grip_data$grip, summary)
tapply(grip_data$ubp, grip_data$grip, sd)
```

Let's visualize the data:

```{r grip_viz, fig.height=4, fig.width=8}
# Boxplot with data points
boxplot(ubp ~ grip, data = grip_data,
        col = c("lightblue", "lightgreen", "lightyellow"),
        main = "Upper Body Power by Grip Type",
        xlab = "Grip Type", ylab = "Upper Body Power")

# Add individual data points
stripchart(ubp ~ grip, data = grip_data, 
           vertical = TRUE, method = "jitter", 
           pch = 16, col = "red", 
           add = TRUE)
```

#### One-Way ANOVA

Now let's perform the ANOVA test:

```{r anova_test}
# Fit the ANOVA model
model <- aov(ubp ~ grip, data = grip_data)

# Display ANOVA summary
summary(model)
```

#### Interpretation

The F-statistic is `r round(summary(model)[[1]][1,4], 3)` with `r summary(model)[[1]][1,1]` and `r summary(model)[[1]][2,1]` degrees of freedom. The p-value is `r round(summary(model)[[1]][1,5], 3)`, which is greater than our significance level of 0.05. Therefore, we fail to reject the null hypothesis.

The data does not provide sufficient evidence to conclude that the mean UBP differs significantly among the three grip types. However, it's worth noting that the p-value is close to 0.05, suggesting there might be some differences that a larger sample size could detect.

#### Post-Hoc Tests

Even though our ANOVA wasn't significant at α = 0.05, we can still explore potential pairwise differences using post-hoc tests. This is useful for understanding which specific groups might differ if we were to collect more data or use a less stringent significance level.

```{r posthoc}
# Tukey's Honest Significant Difference test
TukeyHSD(model)

# Visualize the Tukey HSD results
plot(TukeyHSD(model))
```

The Tukey HSD test confirms that none of the pairwise comparisons are significant at the 0.05 level. The integrated-classic comparison has the smallest adjusted p-value (`r round(TukeyHSD(model)$grip[1,4], 3)`), suggesting that with more data, we might detect a significant difference between these two grip types.

## Checking Assumptions

Hypothesis tests rely on certain assumptions. Let's verify them for our ANOVA example:

### 1. Normality of Residuals

```{r normality_check, fig.height=3.5, fig.width=8}
# Create a layout for two plots side by side
par(mfrow = c(1, 2))

# Histogram of residuals
hist(residuals(model), 
     main = "Histogram of Residuals",
     xlab = "Residuals", 
     breaks = 5, 
     col = "lightblue")

# Q-Q plot of residuals
qqnorm(residuals(model), main = "Q-Q Plot of Residuals")
qqline(residuals(model), col = "red")

# Formal test of normality
shapiro.test(residuals(model))
```

The Shapiro-Wilk test has a p-value of `r round(shapiro.test(residuals(model))$p.value, 3)`, which is greater than 0.05, suggesting the residuals are normally distributed.

### 2. Homogeneity of Variances

```{r variance_check}
# Bartlett's test for homogeneity of variances
bartlett.test(ubp ~ grip, data = grip_data)
```

Bartlett's test has a p-value of `r round(bartlett.test(ubp ~ grip, data = grip_data)$p.value, 3)`, which is greater than 0.05, suggesting the variances are homogeneous across groups.

## Effect Size Calculation

P-values alone don't tell us about the magnitude of differences. Effect sizes help quantify this:

```{r effect_size}
# Calculate eta-squared (proportion of variance explained)
anova_table <- summary(model)[[1]]
eta_squared <- anova_table[1, "Sum Sq"] / sum(anova_table[, "Sum Sq"])
eta_squared
```

The eta-squared value of `r round(eta_squared, 3)` indicates that approximately `r round(eta_squared * 100, 1)`% of the variance in UBP is explained by grip type, which is a moderate effect according to common guidelines.
## Power Analysis

Our ANOVA was not significant, but this could be due to insufficient power with our small sample size:

```{r power_analysis}
# Load the pwr package (install if needed but in a way that doesn't show installation output)
if (!requireNamespace("pwr", quietly = TRUE)) {
  # Use quiet installation that suppresses output
  install.packages("pwr", quiet = TRUE)
}
library(pwr)

# Calculate the observed effect size (Cohen's f)
# f = sqrt(η² / (1 - η²))
f_effect <- sqrt(eta_squared / (1 - eta_squared))

# Calculate power for our current design
power_result <- pwr.anova.test(k = 3,  # Number of groups
                               n = 3,   # Number of observations per group
                               f = f_effect,
                               sig.level = 0.05)

# Calculate required sample size for 80% power
sample_result <- pwr.anova.test(k = 3,  # Number of groups
                                f = f_effect,
                                sig.level = 0.05,
                                power = 0.8)

# Display results in a cleaner way
cat("Power Analysis Results:\n")
cat("---------------------\n")
cat("Effect size (Cohen's f):", round(f_effect, 3), "\n")
cat("Current design power:", round(power_result$power, 3), "\n")
cat("Sample size needed per group for 80% power:", ceiling(sample_result$n), "\n")
```

This analysis suggests our study had only about `r round(power_result$power * 100, 1)`% power to detect the observed effect. To achieve 80% power, we would need approximately `r ceiling(sample_result$n)` observations per group.

## Summary of Hypothesis Testing in R

This document covered:

1. **Unpaired two-sample t-test**: For comparing means of two independent groups
   - Two-sided alternative: Testing for any difference
   - One-sided alternative: Testing for a directional difference

2. **Paired two-sample t-test**: For comparing means of paired observations
   - Accounts for within-subject correlation
   - More powerful than unpaired tests when natural pairing exists

3. **One-way ANOVA**: For comparing means across multiple groups
   - F-statistic tests for any differences among groups
   - Post-hoc tests identify specific group differences

4. **Assumption checking**:
   - Normality of data/residuals
   - Homogeneity of variances

5. **Beyond p-values**:
   - Effect size estimation
   - Power analysis

## Best Practices for Hypothesis Testing

1. **Plan your analysis in advance**:
   - Decide on hypotheses, significance level, and test before collecting data
   - Choose one-sided tests only when there's a strong directional hypothesis

2. **Check assumptions**:
   - Verify normality and variance assumptions
   - Consider transformations or non-parametric alternatives when assumptions are violated

3. **Report completely**:
   - Include test statistics, degrees of freedom, p-values, and effect sizes
   - Report confidence intervals whenever possible

4. **Consider practical significance**:
   - Statistical significance doesn't necessarily imply practical importance
   - Interpret effect sizes in the context of your research domain

5. **Be mindful of multiple testing**:
   - Control for family-wise error rate when conducting multiple tests
   - Consider methods like Bonferroni correction or False Discovery Rate

## Recommended R Packages for Hypothesis Testing

- **stats**: Base R package with t.test(), aov(), etc.
- **car**: For advanced ANOVA and assumption checking
- **emmeans**: For estimated marginal means and comparisons
- **effectsize**: For computing various effect size measures
- **pwr**: For power analysis
- **ggstatsplot**: For creating visualizations with statistical details

```{r closing_plot, fig.height=4, fig.width=9}
# Create a sample visualization as a closing example
if (!requireNamespace("ggplot2", quietly = TRUE)) {
  install.packages("ggplot2", quiet = TRUE)
}
library(ggplot2)

ggplot(pulse_long, aes(x = Sex, y = Pulse, fill = Sex)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, height = 0, size = 2, alpha = 0.7) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "red") +
  labs(title = "Pulse Rate Comparison by Sex",
       subtitle = paste("Two-sample t-test: p =", round(t_test_long$p.value, 3)),
       y = "Pulse Rate",
       caption = "Red diamonds indicate group means") +
  theme_minimal() +
  scale_fill_manual(values = c("pink", "lightblue"))
```