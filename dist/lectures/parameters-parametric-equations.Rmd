---
title: "Parameters, Parametric Equations and Their Role in Frequentist Statistics"
author: "Nathan Ormond"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load necessary packages
library(tidyverse)
library(ggplot2)
library(gridExtra)
```

## Introduction

This document explores the concept of parameters and parametric equations and their fundamental relationship to frequentist statistics. We'll use standard R packages like ggplot2 to visualise these concepts.

> Parameters are essentially variables that are held fixed for a particular instance or application, but whose values could be different in other contexts.

## Parameters in Mathematics

### Mathematical Perspective

Parameters in mathematics are quantities that define a system but remain constant in a specific context:

```{r parameter-functions, fig.height=5}
# Create data for a family of functions
x <- seq(-5, 5, length.out = 100)
parameters <- c(1, 2, 3, 4)

# Create a data frame for plotting
param_df <- expand.grid(x = x, parameter = parameters)
param_df$y <- with(param_df, parameter * x^2)

# Plot the family of functions
ggplot(param_df, aes(x = x, y = y, group = parameter, colour = factor(parameter))) +
  geom_line(size = 1) +
  scale_colour_viridis_d(name = "Parameter a") +
  labs(title = "Family of Quadratic Functions: f(x) = ax²",
       subtitle = "The parameter 'a' determines the shape of the curve",
       x = "x", y = "f(x)") +
  theme_minimal() +
  theme(legend.position = "right")
```

### Parametric Equations

Parametric equations express coordinates as functions of one or more parameters. Here we'll visualize some common parametric curves:

```{r parametric-curves, fig.height=8, fig.width=10}
# Generate parameter values
t <- seq(0, 2*pi, length.out = 200)

# Circle parametric equations
circle_x <- 3 * cos(t)
circle_y <- 3 * sin(t)
circle_df <- data.frame(x = circle_x, y = circle_y, type = "Circle: x = 3cos(t), y = 3sin(t)")

# Spiral parametric equations
spiral_x <- t * cos(t) / 2
spiral_y <- t * sin(t) / 2
spiral_df <- data.frame(x = spiral_x, y = spiral_y, type = "Spiral: x = tcos(t)/2, y = tsin(t)/2")

# Lemniscate parametric equations
lemniscate_x <- 3 * cos(t) / (1 + sin(t)^2)
lemniscate_y <- 3 * sin(t) * cos(t) / (1 + sin(t)^2)
lemniscate_df <- data.frame(x = lemniscate_x, y = lemniscate_y, type = "Lemniscate")

# Plot each curve separately
p1 <- ggplot(circle_df, aes(x = x, y = y)) +
  geom_path(colour = "blue", size = 1) +
  coord_fixed() +
  labs(title = "Circle: x = 3cos(t), y = 3sin(t)") +
  theme_minimal()

p2 <- ggplot(spiral_df, aes(x = x, y = y)) +
  geom_path(colour = "red", size = 1) +
  coord_fixed() +
  labs(title = "Spiral: x = tcos(t)/2, y = tsin(t)/2") +
  theme_minimal()

p3 <- ggplot(lemniscate_df, aes(x = x, y = y)) +
  geom_path(colour = "green4", size = 1) +
  coord_fixed() +
  labs(title = "Lemniscate") +
  theme_minimal()

# Combine into a single grid
grid.arrange(p1, p2, p3, ncol = 2,
             top = "Examples of Parametric Curves")
```

### Parameter Space Visualization

In both mathematics and statistics, we can visualize the concept of a parameter space:

```{r parameter-space, fig.height=5, fig.width=8}
# Create a grid of parameter values
mu <- seq(-3, 3, length.out = 20)
sigma <- seq(0.5, 3, length.out = 20)
params <- expand.grid(mu = mu, sigma = sigma)

# Add a z value for visualization (arbitrary function of parameters)
params$z <- with(params, exp(-(mu^2)/8) * sigma)

# Create a parameter space visualization
p1 <- ggplot(params, aes(x = mu, y = sigma)) +
  geom_tile(aes(fill = z)) +
  scale_fill_viridis_c() +
  labs(title = "Parameter Space Visualization",
       subtitle = "Each point represents a unique combination of parameters",
       x = "μ (location parameter)",
       y = "σ (scale parameter)") +
  theme_minimal()

# Add specific points in parameter space
points <- data.frame(
  mu = c(0, -2, 2, 0),
  sigma = c(1, 1, 1, 2),
  label = c("Standard Normal", "Shifted Left", "Shifted Right", "More Spread")
)

# The key fix: Use the plot without inheriting aesthetics
p2 <- p1 + 
  geom_point(data = points, mapping = aes(x = mu, y = sigma), 
             colour = "red", size = 3, inherit.aes = FALSE) +
  geom_text(data = points, mapping = aes(x = mu, y = sigma, label = label),
            hjust = -0.1, vjust = -0.5, colour = "black", inherit.aes = FALSE)

# Display the plot
p2
```

## Parameters in Statistical Distributions

In probability and statistics, parameters characterise probability distributions:

```{r distribution-params, fig.height=10}
# Normal distribution with different parameters
x_norm <- seq(-4, 4, length.out = 100)
norm_params <- data.frame(
  mean = c(0, 0, -2, 2),
  sd = c(0.5, 1, 1, 1),
  label = c("μ=0, σ=0.5", "μ=0, σ=1", "μ=-2, σ=1", "μ=2, σ=1")
)

norm_df <- expand.grid(x = x_norm, param_id = 1:nrow(norm_params))
norm_df <- merge(norm_df, norm_params, by.x = "param_id", by.y = "row.names")
norm_df$density <- with(norm_df, dnorm(x, mean = mean, sd = sd))

p1 <- ggplot(norm_df, aes(x = x, y = density, colour = label)) +
  geom_line(size = 1) +
  labs(title = "Normal Distribution with Different Parameters",
       x = "x", y = "Density", colour = "Parameters") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Binomial distribution with different parameters
k_binom <- 0:20
binom_params <- data.frame(
  n = c(20, 20, 20, 10),
  p = c(0.3, 0.5, 0.7, 0.5),
  label = c("n=20, p=0.3", "n=20, p=0.5", "n=20, p=0.7", "n=10, p=0.5")
)

binom_df <- expand.grid(k = k_binom, param_id = 1:nrow(binom_params))
binom_df <- merge(binom_df, binom_params, by.x = "param_id", by.y = "row.names")
binom_df$probability <- with(binom_df, dbinom(k, size = n, prob = p))

p2 <- ggplot(binom_df, aes(x = k, y = probability, fill = label)) +
  geom_col(position = "dodge", alpha = 0.7) +
  labs(title = "Binomial Distribution with Different Parameters",
       x = "k (number of successes)", y = "Probability", fill = "Parameters") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Exponential distribution with different parameters
x_exp <- seq(0, 5, length.out = 100)
exp_params <- data.frame(
  rate = c(0.5, 1, 2, 3),
  label = c("λ=0.5", "λ=1", "λ=2", "λ=3")
)

exp_df <- expand.grid(x = x_exp, param_id = 1:nrow(exp_params))
exp_df <- merge(exp_df, exp_params, by.x = "param_id", by.y = "row.names")
exp_df$density <- with(exp_df, dexp(x, rate = rate))

p3 <- ggplot(exp_df, aes(x = x, y = density, colour = label)) +
  geom_line(size = 1) +
  labs(title = "Exponential Distribution with Different λ Parameters",
       x = "x", y = "Density", colour = "Rate Parameter") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Combine plots
grid.arrange(p1, p2, p3, ncol = 1)
```

## Population Parameters vs. Sample Statistics

In statistics, we distinguish between population parameters and sample statistics:

```{r population-sample, fig.height=6}
# Generate a population
set.seed(123)
population <- rnorm(10000, mean = 50, sd = 10)

# Function to take samples and calculate statistics
take_samples <- function(pop, sample_size, n_samples) {
  samples <- replicate(n_samples, sample(pop, sample_size))
  sample_means <- apply(samples, 2, mean)
  data.frame(sample_means = sample_means)
}

# Take samples of different sizes
samples_small <- take_samples(population, 10, 1000)
samples_medium <- take_samples(population, 30, 1000)
samples_large <- take_samples(population, 100, 1000)

# Combine data
samples_df <- bind_rows(
  mutate(samples_small, size = "n = 10"),
  mutate(samples_medium, size = "n = 30"),
  mutate(samples_large, size = "n = 100")
)

# Plot histograms of sample means
ggplot() +
  geom_histogram(data = samples_df, aes(x = sample_means, fill = size), 
                 alpha = 0.5, position = "identity", bins = 30) +
  geom_vline(xintercept = mean(population), colour = "red", linetype = "dashed", size = 1) +
  annotate("text", x = mean(population) + 2, y = 120, 
           label = "Population Mean (μ = 50)", colour = "red") +
  labs(title = "Sampling Distribution of the Mean",
       subtitle = "Sample means approach the population parameter as sample size increases",
       x = "Sample Mean (x̄)", y = "Frequency", fill = "Sample Size") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Parametric Methods in Frequentist Statistics

### Parametric vs. Non-parametric Methods

One way to visualise the distinction between parametric and non-parametric methods is through their reliance on distributional assumptions:

```{r parametric-nonparametric, fig.height=8, fig.width=10}
# Generate data with outliers
set.seed(456)
x <- c(rnorm(40, mean = 10, sd = 2), 25, 26)
y <- c(rnorm(40, mean = 12, sd = 2), 4, 3)

# Create a data frame
data <- data.frame(x = x, y = y)

# Calculate Pearson and Spearman correlations
pearson_cor <- cor(x, y, method = "pearson")
spearman_cor <- cor(x, y, method = "spearman")

# Create scatter plot
p1 <- ggplot(data, aes(x = x, y = y)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", colour = "blue", se = FALSE) +
  labs(title = "Parametric (Pearson's r) vs. Non-parametric (Spearman's rho)",
       subtitle = paste0("Pearson's r = ", round(pearson_cor, 2), 
                        ", Spearman's rho = ", round(spearman_cor, 2)),
       x = "X", y = "Y") +
  theme_minimal()

# Generate normal and non-normal data
set.seed(789)
normal_data <- rnorm(200, mean = 0, sd = 1)
nonnormal_data <- c(rexp(150, rate = 1), rnorm(50, mean = 4, sd = 1))

# Create a data frame for density plots
density_df <- data.frame(
  value = c(normal_data, nonnormal_data),
  distribution = rep(c("Normal", "Non-normal"), each = 200)
)

# Create density plots
p2 <- ggplot(density_df, aes(x = value, fill = distribution)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution Shapes: Normal vs. Non-normal",
       subtitle = "Parametric tests assume normality for optimal performance",
       x = "Value", y = "Density") +
  theme_minimal()

# Combine plots
grid.arrange(p1, p2, ncol = 1)
```

### Effect of Parametric Assumptions

The following visualization demonstrates how parametric assumptions affect statistical inference:

```{r parametric-assumptions, fig.height=8, fig.width=10}
# Generate two groups with same mean but different distributions
set.seed(123)
norm_group <- rnorm(100, mean = 10, sd = 2)
skew_group <- 10 + rexp(100, rate = 0.5) - 2  # Adjusted to have mean ≈ 10

# Combine data
group_df <- data.frame(
  value = c(norm_group, skew_group),
  group = rep(c("Normal", "Skewed"), each = 100)
)

# Visualize distributions
p3 <- ggplot(group_df, aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean(norm_group), linetype = "dashed", color = "blue") +
  geom_vline(xintercept = mean(skew_group), linetype = "dashed", color = "red") +
  annotate("text", x = mean(norm_group) + 1, y = 0.25, 
           label = paste("Mean =", round(mean(norm_group), 2)), color = "blue") +
  annotate("text", x = mean(skew_group) + 1, y = 0.2, 
           label = paste("Mean =", round(mean(skew_group), 2)), color = "red") +
  labs(title = "Different Distributions with Similar Means",
       subtitle = "Parametric tests focus on means, non-parametric on distribution shape") +
  theme_minimal()

# Create QQ plots for both groups
norm_qq <- ggplot(data.frame(x = norm_group), aes(sample = x)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "QQ Plot: Normal Group") +
  theme_minimal()

skew_qq <- ggplot(data.frame(x = skew_group), aes(sample = x)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "QQ Plot: Skewed Group") +
  theme_minimal()

# Add QQ plot explanation text
p_explanation <- ggplot() + 
  annotate("rect", xmin = -1, xmax = 1, ymin = -1, ymax = 1, 
           fill = "lightblue", alpha = 0.3) +
  annotate("text", x = 0, y = 0, 
           label = "QQ (Quantile-Quantile) plots compare sample quantiles with theoretical quantiles\nfrom a normal distribution. Points following the straight line indicate normality.\nDeviations from the line, especially at the ends, suggest non-normality.\nThe skewed group's QQ plot shows clear departure from the line,\nindicating a violation of the normality assumption.",
           size = 3.5, hjust = 0.5, vjust = 0.5) +
  theme_void() +
  labs(title = "Understanding QQ Plots")

# Combine plots
grid.arrange(p3, norm_qq, skew_qq, p_explanation, 
             ncol = 2, heights = c(2, 1))
```

## Parameters in Null Hypothesis Significance Testing (NHST)

### Hypothesis Testing as Parameter Estimation

In frequentist statistics, hypothesis testing involves making claims about population parameters:

```{r nhst-parameters, fig.height=7, fig.width=10}
# Generate some data where H0 is false
set.seed(123)
sample_data <- rnorm(30, mean = 2, sd = 1)

# Perform one-sample t-test
t_test_res <- t.test(sample_data, mu = 0)

# Create data for null hypothesis distribution
null_dist <- data.frame(
  x = seq(-3, 5, length.out = 200),
  null = dnorm(seq(-3, 5, length.out = 200), mean = 0, sd = 1/sqrt(30)),
  alternative = dnorm(seq(-3, 5, length.out = 200), mean = mean(sample_data), sd = sd(sample_data)/sqrt(30))
)

# Create plot of null and alternative distributions
p6 <- ggplot(null_dist, aes(x = x)) +
  geom_line(aes(y = null, colour = "Null Hypothesis (μ = 0)")) +
  geom_line(aes(y = alternative, colour = "Observed Data")) +
  geom_vline(xintercept = mean(sample_data), linetype = "dashed", colour = "red") +
  annotate("text", x = mean(sample_data) + 0.3, y = 1.5, 
           label = paste("Sample Mean =", round(mean(sample_data), 2))) +
  labs(title = "Parameter Distributions: Null Hypothesis vs. Observed Data",
       subtitle = paste("One-sample t-test p-value:", format(t_test_res$p.value, scientific = TRUE)),
       x = "Parameter Value (μ)", y = "Density", colour = "Distribution") +
  theme_minimal()

# Create confidence interval visualization
ci <- t_test_res$conf.int
ci_df <- data.frame(
  x = seq(ci[1] - 0.5, ci[2] + 0.5, length.out = 200),
  y = dnorm(seq(ci[1] - 0.5, ci[2] + 0.5, length.out = 200), 
            mean = mean(sample_data), sd = sd(sample_data)/sqrt(30))
)

p7 <- ggplot(ci_df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(ci_df, x >= ci[1] & x <= ci[2]), 
            aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = mean(sample_data), colour = "red") +
  geom_vline(xintercept = 0, colour = "black", linetype = "dashed") +
  annotate("text", x = ci[1], y = max(ci_df$y) * 0.8, label = round(ci[1], 2), hjust = 1) +
  annotate("text", x = ci[2], y = max(ci_df$y) * 0.8, label = round(ci[2], 2), hjust = 0) +
  labs(title = "95% Confidence Interval for the Population Mean",
       subtitle = "Note that the null value (μ = 0) lies outside the interval",
       x = "Parameter Value (μ)", y = "Density") +
  theme_minimal()

# Combine plots
grid.arrange(p6, p7, ncol = 1)
```

## Parameter Estimation Methods

### Maximum Likelihood Estimation (MLE)

One of the most important parameter estimation methods is Maximum Likelihood Estimation:

```{r mle-visualization, fig.height=6, fig.width=9}
# Generate sample data from normal distribution
set.seed(123)
sample_data <- rnorm(50, mean = 5, sd = 2)

# Function to calculate log-likelihood for normal distribution
log_lik <- function(mu, sigma, data) {
  sum(dnorm(data, mean = mu, sd = sigma, log = TRUE))
}

# Create grid of parameter values
mu_grid <- seq(3, 7, length.out = 50)
sigma_grid <- seq(1, 3, length.out = 50)
params_grid <- expand.grid(mu = mu_grid, sigma = sigma_grid)

# Calculate log-likelihood for each parameter combination
params_grid$loglik <- mapply(function(mu, sigma) {
  log_lik(mu, sigma, sample_data)
}, params_grid$mu, params_grid$sigma)

# Find MLE estimates
mle_mu <- mean(sample_data)
mle_sigma <- sd(sample_data)

# Create contour plot of log-likelihood
p8 <- ggplot(params_grid, aes(x = mu, y = sigma, z = loglik)) +
  geom_contour_filled() +
  geom_point(data = data.frame(mu = mle_mu, sigma = mle_sigma), 
             mapping = aes(x = mu, y = sigma), 
             colour = "red", size = 3, inherit.aes = FALSE) +
  annotate("text", x = mle_mu, y = mle_sigma - 0.15, 
           label = paste("MLE: μ =", round(mle_mu, 2), ", σ =", round(mle_sigma, 2))) +
  labs(title = "Maximum Likelihood Estimation",
       subtitle = "Contour plot of log-likelihood for different parameter values",
       x = "μ (Mean Parameter)", y = "σ (Standard Deviation Parameter)") +
  theme_minimal()

# Create profile likelihood for mean parameter
profile_mu <- data.frame(
  mu = mu_grid,
  loglik = sapply(mu_grid, function(mu) {
    log_lik(mu, mle_sigma, sample_data)
  })
)

p9 <- ggplot(profile_mu, aes(x = mu, y = loglik)) +
  geom_line() +
  geom_vline(xintercept = mle_mu, colour = "red", linetype = "dashed") +
  labs(title = "Profile Likelihood for Mean Parameter",
       subtitle = "Fixed σ at MLE value",
       x = "μ (Mean Parameter)", y = "Log-Likelihood") +
  theme_minimal()

# Combine plots
grid.arrange(p8, p9, ncol = 2)
```

## Geometric Interpretation of Statistical Tests


## Connection to Bayesian Framework

While the focus is on frequentist statistics, it's instructive to briefly contrast with the Bayesian approach to parameters:

```{r bayesian-contrast, fig.height=6, fig.width=10}
# Generate some sample data
set.seed(101)
sample_data <- rbinom(20, size = 1, prob = 0.7)
n_success <- sum(sample_data)
n_trials <- length(sample_data)

# Create prior, likelihood, and posterior for a binomial parameter
theta <- seq(0, 1, length.out = 100)

# Frequentist approach (likelihood only)
likelihood <- dbeta(theta, n_success + 1, n_trials - n_success + 1)

# Bayesian approach with different priors
weak_prior <- dbeta(theta, 2, 2)  # Weak prior centered at 0.5
strong_prior <- dbeta(theta, 10, 5)  # Strong prior centered at 0.67

# Calculate posteriors
weak_posterior <- dbeta(theta, n_success + 2, n_trials - n_success + 2)
strong_posterior <- dbeta(theta, n_success + 10, n_trials - n_success + 5)

# Create data frame for plotting
bayes_df <- data.frame(
  theta = rep(theta, 4),
  density = c(likelihood, weak_prior, weak_posterior, strong_posterior),
  type = rep(c("Likelihood (Frequentist)", "Weak Prior", 
               "Posterior with Weak Prior", "Posterior with Strong Prior"), 
             each = 100)
)

# Plot comparison
ggplot(bayes_df, aes(x = theta, y = density, color = type)) +
  geom_line(size = 1) +
  geom_vline(xintercept = n_success/n_trials, linetype = "dashed") +
  annotate("text", x = n_success/n_trials + 0.05, y = max(likelihood) * 0.5, 
           label = paste("MLE =", n_success/n_trials)) +
  labs(title = "Parameter Estimation: Frequentist vs. Bayesian",
       subtitle = "Frequentist inference relies solely on the likelihood function",
       x = "θ (Probability Parameter)", y = "Density", color = "Distribution") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## The Mathematical Foundations of Parametric Methods

### Statistical Models as Parametric Equations

Statistical models can be viewed as parametric equations with an added stochastic component:

```{r stat-models, fig.height=6, fig.width=8}
# Generate data points with error term
set.seed(123)
x_vals <- seq(0, 10, length.out = 50)
y_true <- 2 + 3 * x_vals  # True model: y = 2 + 3x
y_obs <- y_true + rnorm(length(x_vals), mean = 0, sd = 5)  # Add random error

# Create data frame
model_df <- data.frame(
  x = x_vals,
  y_true = y_true,
  y_obs = y_obs
)

# Plot true model vs observed data
ggplot(model_df, aes(x = x)) +
  geom_line(aes(y = y_true, color = "True Model"), size = 1) +
  geom_point(aes(y = y_obs, color = "Observed Data"), alpha = 0.7) +
  geom_smooth(aes(y = y_obs, color = "Estimated Model"), method = "lm", se = FALSE) +
  scale_color_manual(values = c("True Model" = "blue", 
                               "Observed Data" = "black",
                               "Estimated Model" = "red")) +
  labs(title = "Statistical Models as Parametric Equations",
       subtitle = "True model: y = 2 + 3x, Observed data includes random error",
       x = "x", y = "y", color = "Component") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

### The Role of the Central Limit Theorem

The Central Limit Theorem (CLT) is a foundational concept that underpins many parametric methods:

```{r clt-demo, fig.height=7, fig.width=10}
# Demonstrate the CLT with different distributions
set.seed(456)

# Generate samples from different distributions
n_samples <- 1000
sample_size <- 30

# Uniform distribution
unif_samples <- replicate(n_samples, mean(runif(sample_size, 0, 10)))

# Exponential distribution
exp_samples <- replicate(n_samples, mean(rexp(sample_size, rate = 0.5)))

# Binomial distribution
binom_samples <- replicate(n_samples, mean(rbinom(sample_size, size = 1, prob = 0.3)))

# Create data frame
clt_df <- data.frame(
  sample_mean = c(unif_samples, exp_samples, binom_samples),
  distribution = rep(c("Uniform", "Exponential", "Binomial"), each = n_samples)
)

# Plot sampling distributions
ggplot(clt_df, aes(x = sample_mean, fill = distribution)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ distribution, scales = "free") +
  labs(title = "The Central Limit Theorem in Action",
       subtitle = "Sampling distributions of means approach normality regardless of the original distribution",
       x = "Sample Mean", y = "Density") +
  theme_minimal() +
  theme(legend.position = "none")
```

## Understanding Parameters in the Context of Statistical Tests

To better understand the connection between parameters and statistical tests, let's create a visual comparison:

```{r test-parameters, fig.height=5, fig.width=10}
# Create a data frame of tests and their parameters
test_params <- data.frame(
  Test = c("t-test", "ANOVA", "Chi-square", "Correlation", "Regression"),
  Parameter = c("Mean (μ)", "Means (μ₁, μ₂, ...)", "Proportions (p₁, p₂, ...)", 
               "Correlation (ρ)", "Coefficients (β₀, β₁, ...)"),
  NullHypothesis = c("μ = μ₀", "μ₁ = μ₂ = ... = μₖ", "p₁ = p₂ = ... = pₖ", 
                     "ρ = 0", "β₁ = 0"),
  Assumptions = c("Normality", "Normality, Equal variances", "Independence", 
                 "Bivariate normality", "Linearity, Independence, Homoscedasticity")
)

# Display table
knitr::kable(test_params, caption = "Parameters in Common Statistical Tests")
```

### Visual Summary of Key Concepts

```{r visual-summary, fig.height=8, fig.width=10}
# Create data for a visual summary
concepts <- data.frame(
  x = c(1, 2, 3, 1, 2, 3),
  y = c(3, 3, 3, 2, 2, 2),
  concept = c("Parameters in\nMathematics", "Parameters in\nProbability", "Parameters in\nInference",
              "Parametric\nEquations", "Probability\nDistributions", "Statistical\nTests"),
  description = c("Define relationships\nand shapes", "Characterize distribution\nproperties", "Targets of estimation\nand testing",
                 "x = f(t), y = g(t)", "Normal: μ, σ\nBinomial: n, p", "H₀: Parameter = value\nCI: Parameter range")
)

# Create visual summary
ggplot(concepts, aes(x = x, y = y)) +
  # Add background rectangles, using inherit.aes = FALSE to avoid errors
  geom_rect(mapping = aes(xmin = x - 0.4, xmax = x + 0.4, ymin = y - 0.4, ymax = y + 0.4),
            fill = "lightblue", alpha = 0.5) +
  # Add text labels, again using inherit.aes = FALSE
  geom_text(mapping = aes(label = concept), fontface = "bold") +
  geom_text(mapping = aes(label = description, y = y - 0.25), size = 3) +
  # Add connecting lines
  geom_segment(aes(x = 1, y = 3, xend = 1, yend = 2.4), arrow = arrow(length = unit(0.2, "cm"))) +
  geom_segment(aes(x = 2, y = 3, xend = 2, yend = 2.4), arrow = arrow(length = unit(0.2, "cm"))) +
  geom_segment(aes(x = 3, y = 3, xend = 3, yend = 2.4), arrow = arrow(length = unit(0.2, "cm"))) +
  # Set theme and labels
  theme_void() +
  labs(title = "Parameters: The Unifying Concept in Mathematics and Statistics",
       subtitle = "From mathematical definitions to statistical inference") +
  theme(plot.title = element_text(hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5))
```

## Summary and Conclusion

### Key Concepts About Parameters in Statistics

Parameters play a central role in both mathematics and statistics:

1. **Mathematical foundation**: Parameters define functional relationships and geometric shapes
2. **Statistical distributions**: Parameters characterize probability distributions (mean, variance, etc.)
3. **Hypothesis testing**: Statistical inference revolves around estimating and testing population parameters
4. **Parametric vs. non-parametric**: The distinction reflects different assumptions about parameters

### The Parametric Framework in Frequentist Statistics

Frequentist statistics is fundamentally parametric in its approach:

1. **Fixed but unknown parameters**: Population parameters are considered fixed constants
2. **Point and interval estimation**: Methods like MLE provide estimates with quantified uncertainty
3. **Hypothesis tests**: Formal procedures to assess claims about parameter values
4. **Distributional assumptions**: Parametric methods assume specific probability distributions
5. **Sampling distributions**: Distributions of sample statistics depend on population parameters

Understanding parameters and their role in frequentist statistics provides a deeper appreciation of statistical methods and their mathematical foundations. The concept of parameters bridges mathematical expressions, probability distributions, and statistical inference, creating a coherent framework for data analysis and interpretation.